# Data2AITextbook

## ðŸŽ¯ Goal

> Automatically convert unstructured data into a high-quality 'textbook' format, optimized for fine-tuning Large Language Models (LLMs) ðŸš€

## About

Inspired by [Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644.pdf), which produced `phi-1` LLM trained with "textbook quality" data.

### Transforming > Filtering

We want to maximize what can be learned from any input dataset, therefore instead of filtering to exclude poor quality data we try to transform poor to excellent quality data ðŸ“ˆ

### Roadmap

Given an input text, such as a chat message, blog post, article, speech/video (transcribed to text), etc extract the following and generate an effective fine-tuning solution (e.g. dataset, etc):

- [ ] ðŸš§ Factual Knowledge
    - [x] Questions Generation ([#1](https://github.com/Glavin001/Data2AITextbook/issues/1))
        - [x] Paraphrasing ([#4](https://github.com/Glavin001/Data2AITextbook/issues/4))
    - [ ] ðŸš§ Answer Generation ([#5](https://github.com/Glavin001/Data2AITextbook/issues/5))
    - [ ] Rules (e.g. when X then Y, etc)
    - [ ] State Charts (e.g. actions that can be taken in the world, preconditions, results and side-effects, costs, etc)
- [ ] Meta-cognitive
  - [ ] Self-monitoring
  - [ ] Strategies for reasoning
  - [ ] Orchestration of planning
  - [ ] Communication styles (e.g. personality, word choice, etc)

## Models

Coming soon.
