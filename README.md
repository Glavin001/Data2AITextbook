# Data2AITextbook

## ðŸŽ¯ Goal

> Automatically convert unstructured data into a high-quality 'textbook' format, optimized for fine-tuning Large Language Models (LLMs) ðŸš€

## About

Inspired by [Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644.pdf), which produced `phi-1` LLM trained with "textbook quality" data.

### Transforming > Filtering

We want to maximize what can be learned from any input dataset, therefore instead of filtering to exclude poor quality data we try to transform poor to excellent quality data ðŸ“ˆ

### Roadmap

Given an input text, such as a chat message, blog post, article, speech/video (transcribed to text), etc extract the following and generate an effective fine-tuning solution (e.g. dataset, etc):

- [ ] ðŸš§ Factual Knowledge
    - [x] Questions Generation ([#1](https://github.com/Glavin001/Data2AITextbook/issues/1))
        - [x] Paraphrasing ([#4](https://github.com/Glavin001/Data2AITextbook/issues/4))
    - [ ] ðŸš§ Answer Generation ([#5](https://github.com/Glavin001/Data2AITextbook/issues/5))
    - [ ] Rules (e.g. when X then Y, etc)
    - [ ] State Charts (e.g. actions that can be taken in the world, preconditions, results and side-effects, costs, etc)
- [ ] Meta-cognitive
  - [ ] Self-monitoring
  - [ ] Strategies for reasoning
  - [ ] Orchestration of planning
  - [ ] Communication styles (e.g. personality, word choice, etc)

## Datasets & Models

Check out my Huggingface profile for a list of datasets & models I've created: https://huggingface.co/Glavin001

Many will be created for the [Expertise by AI](https://github.com/Glavin001/Expertise-by-AI) project, where you can learn how to train custom models with your own data.
